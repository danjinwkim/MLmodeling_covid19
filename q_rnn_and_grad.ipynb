{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danjinwkim/MLmodeling_covid19/blob/main/q_rnn_and_grad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIqzcT6vyUJI"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, we'll implement simple RNNs and LSTMs, then explore how gradients flow through these different networks.\n",
        "\n",
        "This notebook does not require a Colab GPU. If it's enabled, you can turn it off through Runtime -> Change runtime type. (This will make it more likely for you to get Colab GPU access later in the REAL_RNN_LSTM.ipynb problem.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL29FMhfyUJv"
      },
      "source": [
        "# Imports\n",
        "\n",
        "Note: the ipympl installation will require you to restart the colab runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4YRmWnXe1-c"
      },
      "outputs": [],
      "source": [
        "! pip install ipympl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G1WS5wsd2r4"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "# If you are not using colab you can delete these two lines\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "import torch as th\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interactive, widgets, Layout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Isyir7ROd20g"
      },
      "outputs": [],
      "source": [
        "%matplotlib ipympl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WmHtmlVul1z"
      },
      "source": [
        "# 1.A: implementing a RNN layer\n",
        "\n",
        "Consider using Pytorch's [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear). You can implement this with either one Linear layer or two. If you use two, remember that you only need to include a bias term for one of the linear layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXsJZLtepGYn"
      },
      "outputs": [],
      "source": [
        "class RNNLayer(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, nonlinearity=th.tanh):\n",
        "    \"\"\"\n",
        "    Initialize a single RNN layer.\n",
        "\n",
        "    Inputs:\n",
        "    - input_size: Data input feature dimension\n",
        "    - hidden_size: RNN hidden state size (also the output feature dimension)\n",
        "    - nonlinearity: Nonlinearity applied to the rnn output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.nonlinearity = nonlinearity\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize any parameters your class needs.                          #\n",
        "    ##############################################################################\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    RNN forward pass\n",
        "\n",
        "    Inputs:\n",
        "    - x: input tensor (B, seq_len, input_size)\n",
        "\n",
        "    Returns:\n",
        "    - all_h: tensor of size (B, seq_len, hidden_size) containing hidden states\n",
        "             produced for each timestep\n",
        "    - last_h: hidden state from the last timestep (B, hidden_size)\n",
        "    \"\"\"\n",
        "    h_list = []  # List to store the hidden states [h_1, ... h_T]\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the RNN forward step                                       #\n",
        "    # 1. Initialize h0 with zeros                                                #\n",
        "    # 2. Roll out the RNN over the sequence, storing hidden states in h_list     #\n",
        "    # 3. Return the appropriate outputs                                          #\n",
        "    ##############################################################################\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "    # h_list should now contain all hidden states, each of size (B, hidden_size)\n",
        "    # We will store the hidden states so we can analyze their gradients later\n",
        "    self.store_h_for_grad(h_list)\n",
        "    all_h = th.stack(h_list, dim=1)\n",
        "    return all_h, last_h\n",
        "\n",
        "  def store_h_for_grad(self, h_list):\n",
        "    \"\"\"\n",
        "    Store input list and allow gradient computation for all list elements\n",
        "    \"\"\"\n",
        "    for h in h_list:\n",
        "      h.retain_grad()\n",
        "    self.h_list = h_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usA4sa1CDzOs"
      },
      "source": [
        "### Test Cases\n",
        "\n",
        "If your implementation is correct, you should expect to see errors of less than 1e-4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6mwjkpMyt6n"
      },
      "outputs": [],
      "source": [
        "rnn = RNNLayer(1, 1)\n",
        "# Overwrite initial parameters with fixed values.\n",
        "# Should give deterministic results even with different implementations.\n",
        "rnn.load_state_dict({k: v * 0 + .1 for k, v in rnn.state_dict().items()})\n",
        "data = th.ones((1, 1, 1))\n",
        "expected_out = th.FloatTensor([[[0.1973753273487091]]])\n",
        "all_h, last_h = rnn(data)\n",
        "assert all_h.shape == expected_out.shape\n",
        "assert th.all(th.isclose(all_h, last_h))\n",
        "print(f'Expected: {expected_out.item()}, got: {last_h.item()}, max error: {th.max(th.abs(expected_out - last_h)).item()}')\n",
        "\n",
        "rnn = RNNLayer(2, 3, nonlinearity=lambda x: x)  # no nonlinearity\n",
        "\n",
        "num_params = sum(p.numel() for p in rnn.parameters())\n",
        "assert num_params == 18, f'expected 18 parameters but found {num_params}'\n",
        "\n",
        "rnn.load_state_dict({k: v * 0 - .1 for k, v in rnn.state_dict().items()})\n",
        "data = th.FloatTensor([[[.1, .15], [.2, .25], [.3, .35], [.4, .45]], [[-.1, -1.5], [-.2, -2.5], [-.3, -3.5], [-.4, -.45]]])\n",
        "expected_all_h = th.FloatTensor([[[-0.1250, -0.1250, -0.1250],\n",
        "         [-0.1075, -0.1075, -0.1075],\n",
        "         [-0.1328, -0.1328, -0.1328],\n",
        "         [-0.1452, -0.1452, -0.1452]],\n",
        "\n",
        "        [[ 0.0600,  0.0600,  0.0600],\n",
        "         [ 0.1520,  0.1520,  0.1520],\n",
        "         [ 0.2344,  0.2344,  0.2344],\n",
        "         [-0.0853, -0.0853, -0.0853]]])\n",
        "expected_last_h = th.FloatTensor([[-0.1452, -0.1452, -0.1452],\n",
        "        [-0.0853, -0.0853, -0.0853]])\n",
        "all_h, last_h = rnn(data)\n",
        "assert all_h.shape == expected_all_h.shape\n",
        "assert last_h.shape == expected_last_h.shape\n",
        "print(f'Max error all_h: {th.max(th.abs(expected_all_h - all_h)).item()}')\n",
        "print(f'Max error last_h: {th.max(th.abs(expected_last_h - last_h)).item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbZtognVqQEO"
      },
      "source": [
        "# 1.B Implementing a RNN regression model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFXhxs9UqQOS"
      },
      "outputs": [],
      "source": [
        "class RecurrentRegressionModel(nn.Module):\n",
        "  def __init__(self, recurrent_net, output_dim=1):\n",
        "    \"\"\"\n",
        "    Initialize a simple RNN regression model\n",
        "\n",
        "    Inputs:\n",
        "    - recurrent_net: an RNN or LSTM (single or multi layer)\n",
        "    - output_dim: feature dimension of the output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.recurrent_net = recurrent_net\n",
        "    self.output_dim = output_dim\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize any parameters you need                                   #\n",
        "    # HINT: use recurrent_net.hidden_size to find the hidden state size          #\n",
        "    ##############################################################################\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass\n",
        "\n",
        "    Inputs:\n",
        "    - x: input tensor (B, seq_len, input_size)\n",
        "\n",
        "    Returns:\n",
        "    - out: predictions of shape (B, seq_len, self.output_dim).\n",
        "    - all_h: tensor of size (B, seq_len, hidden_size) containing hidden states\n",
        "             produced for each timestep.\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward step.                                          #\n",
        "    ##############################################################################\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return out, all_h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHVeC-hCSA6S"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWVnBJUbSBFP"
      },
      "outputs": [],
      "source": [
        "rnn = RecurrentRegressionModel(RNNLayer(2, 3), 4)\n",
        "\n",
        "num_params = sum(p.numel() for p in rnn.parameters())\n",
        "assert num_params == 34, f'expected 34 parameters but found {num_params}'\n",
        "\n",
        "rnn.load_state_dict({k: v * 0 - .1 for k, v in rnn.state_dict().items()})\n",
        "data = th.FloatTensor([[[.1, .15], [.2, .25], [.3, .35], [.4, .45]], [[-.1, -1.5], [-.2, -2.5], [-.3, -3.5], [-.4, -.45]]])\n",
        "expected_preds = th.FloatTensor([[[-0.0627, -0.0627, -0.0627, -0.0627],\n",
        "         [-0.0678, -0.0678, -0.0678, -0.0678],\n",
        "         [-0.0604, -0.0604, -0.0604, -0.0604],\n",
        "         [-0.0567, -0.0567, -0.0567, -0.0567]],\n",
        "\n",
        "        [[-0.1180, -0.1180, -0.1180, -0.1180],\n",
        "         [-0.1453, -0.1453, -0.1453, -0.1453],\n",
        "         [-0.1692, -0.1692, -0.1692, -0.1692],\n",
        "         [-0.0748, -0.0748, -0.0748, -0.0748]]])\n",
        "expected_all_h = th.FloatTensor([[[-0.1244, -0.1244, -0.1244],\n",
        "         [-0.1073, -0.1073, -0.1073],\n",
        "         [-0.1320, -0.1320, -0.1320],\n",
        "         [-0.1444, -0.1444, -0.1444]],\n",
        "\n",
        "        [[ 0.0599,  0.0599,  0.0599],\n",
        "         [ 0.1509,  0.1509,  0.1509],\n",
        "         [ 0.2305,  0.2305,  0.2305],\n",
        "         [-0.0840, -0.0840, -0.0840]]])\n",
        "preds, all_h = rnn(data)\n",
        "assert all_h.shape == expected_all_h.shape\n",
        "assert preds.shape == expected_preds.shape\n",
        "print(f'Max error all_h: {th.max(th.abs(expected_all_h - all_h)).item()}')\n",
        "print(f'Max error last_h: {th.max(th.abs(expected_preds - preds)).item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC9LWlnOKyKi"
      },
      "source": [
        "# Problem 1.C: Dataset and loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZZ26gO5wYAH"
      },
      "source": [
        "## 1.C.i: Understanding the dataset (no implementation needed)\n",
        "\n",
        "Inspect the code and plots below to visualize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeJgsS0F_p8Q"
      },
      "outputs": [],
      "source": [
        "def generate_batch(seq_len=10, batch_size=1):\n",
        "  data = th.randn(size=(batch_size, seq_len, 1))\n",
        "  sums = th.cumsum(data, dim=1)\n",
        "  div = (th.arange(seq_len) + 1).unsqueeze(0).unsqueeze(2)\n",
        "  target = sums / div\n",
        "  return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7iiiEsV_67L"
      },
      "outputs": [],
      "source": [
        "x, y = generate_batch(seq_len=10, batch_size=4)\n",
        "for i in range(4):\n",
        "  fig, ax1 = plt.subplots(1)\n",
        "  ax1.plot(x[i, :, 0])\n",
        "  ax1.plot(y[i, :, 0])\n",
        "  ax1.legend(['x', 'y'])\n",
        "  plt.title('Targets at all timesteps')\n",
        "  plt.show()\n",
        "\n",
        "for i in range(4):\n",
        "  fig, ax1 = plt.subplots(1)\n",
        "  ax1.plot(x[i, :, 0])\n",
        "  ax1.plot(np.arange(10), [y[i, -1].item()] * 10)\n",
        "  ax1.legend(['x', 'y'])\n",
        "  plt.title('Predict only at the last timestep')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLLCIL7iH4CE"
      },
      "source": [
        "## 1.C.ii Implement the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2g0Nc4SVIjS"
      },
      "outputs": [],
      "source": [
        "def loss_fn(pred, y, last_timestep_only=False):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "  - pred: model predictions of size (batch, seq_len, 1)\n",
        "  - y: targets of size (batch, seq_len, 1)\n",
        "  - last_timestep_only: boolean indicating whether to compute loss for all\n",
        "    timesteps or only the lat\n",
        "\n",
        "  Returns:\n",
        "  - loss: scalar MSE loss between pred and true labels\n",
        "  \"\"\"\n",
        "  ##############################################################################\n",
        "  # TODO: implement the loss (HINT: look for pytorch's MSELoss function)       #\n",
        "  ##############################################################################\n",
        "  ##############################################################################\n",
        "  #                               END OF YOUR CODE                             #\n",
        "  ##############################################################################\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLV9IqVOWnoC"
      },
      "source": [
        "### Tests\n",
        "You should see errors < 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP9pnOItWoFA"
      },
      "outputs": [],
      "source": [
        "pred = th.FloatTensor([[.1, .2, .3], [.4, .5, .6]])\n",
        "y = th.FloatTensor([[-1.1, -1.2, -1.3], [-1.4, -1.5, -1.6]])\n",
        "loss_all = loss_fn(pred, y, last_timestep_only=False)\n",
        "loss_last = loss_fn(pred, y, last_timestep_only=True)\n",
        "assert loss_all.shape == loss_last.shape == th.Size([])\n",
        "print(f'Max error loss_all: {th.abs(loss_all - th.tensor(3.0067)).item()}')\n",
        "print(f'Max error loss_last: {th.abs(loss_last - th.tensor(3.7)).item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiNUuKSw3glG"
      },
      "source": [
        "# 1.D: Analyzing RNN Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaaGVQyejUjE"
      },
      "source": [
        "You do not need to understand the details of the GradientVisualizer class in order to complete this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N80gvsMvK78G"
      },
      "outputs": [],
      "source": [
        "def biggest_eig_magnitude(matrix):\n",
        "  \"\"\"\n",
        "  Inputs: a square matrix\n",
        "  Returns: the scalar magnitude of the largest eigenvalue\n",
        "  \"\"\"\n",
        "  h, w = matrix.shape\n",
        "  assert h == w, f'Matrix has shape {matrix.shape}, but eigenvalues can only be computed for square matrices'\n",
        "  eigs = th.linalg.eigvals(matrix)\n",
        "  eig_magnitude = eigs.abs()\n",
        "  eigs_sorted = sorted([i.item() for i in eig_magnitude], reverse=True)\n",
        "  first_eig_magnitude = eigs_sorted[0]\n",
        "  return first_eig_magnitude\n",
        "\n",
        "class GradientVisualizer:\n",
        "\n",
        "  def __init__(self, rnn, last_timestep_only):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - rnn: rnn module\n",
        "    - last_timestep_only: boolean indicating whether to compute loss for all\n",
        "      timesteps or only the lat\n",
        "\n",
        "    Returns:\n",
        "    - loss: scalar MSE loss between pred and true labels\n",
        "    \"\"\"\n",
        "\n",
        "    self.rnn = rnn\n",
        "    self.last_timestep_only = last_timestep_only\n",
        "    self.model = RecurrentRegressionModel(rnn)\n",
        "    self.original_weights = copy.deepcopy(rnn.state_dict())\n",
        "\n",
        "    # Generate a single batch to be used repeatedly\n",
        "    self.x, self.y = generate_batch(seq_len=10)\n",
        "    print(f'Data point: x={np.round(self.x[0, :, 0].detach().cpu().numpy(), 2)}, y={np.round(self.y.squeeze().detach().cpu().numpy(), 2)}')\n",
        "\n",
        "  def plot_visuals(self):\n",
        "    \"\"\" Generate plots which will be updated in realtime.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    ax1.set_title('RNN Outputs')\n",
        "    ax1.set_xlabel('Unroll Timestep')\n",
        "    ax1.set_ylabel('Hidden State Norm')\n",
        "    ax1.set_ylim(-1, 5)\n",
        "    plt_1 = ax1.plot(np.arange(1, 11), np.zeros(10) + 1)  # placeholder vals\n",
        "    plt_1 = plt_1[0]\n",
        "\n",
        "    ax2.set_title('Gradients')\n",
        "    ax2.set_xlabel('Unroll Timestep')\n",
        "    ax2.set_ylabel('RNN dLoss/d a_t Gradient Magitude')\n",
        "    ax2.set_ylim( (10**-6,1e5) )\n",
        "    ax2.set_yscale('log')\n",
        "    # X-axis labels are reversed since the gradient flow is from later layers to earlier layers\n",
        "    ax2.set_xticks(np.arange(10), np.arange(10, 0, -1))\n",
        "    plt_2 = ax2.plot(np.arange(10), np.arange(10) + 1)  # placeholder vals\n",
        "    plt_2 = plt_2[0]\n",
        "    self.fig = fig\n",
        "    self.plots = [plt_1, plt_2]\n",
        "    return plt_1, plt_2, fig\n",
        "\n",
        "  # Main update function for interactive plot\n",
        "  def update_plots(self, weight_val=0, bias_val=0):\n",
        "    # Scale the original RNN weights by a constant\n",
        "    w_dict = copy.deepcopy(self.original_weights)\n",
        "    ##############################################################################\n",
        "    # TODO: Scale all W matrixes by weight_val, and all bias matrices by bias_val#\n",
        "    # If you're using PyTorch nn.Linear layers, you don't need to modify the code#\n",
        "    # provided, but if you're using custom layers, modify this block.            #\n",
        "    ##############################################################################\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    self.rnn.load_state_dict(w_dict)\n",
        "\n",
        "    # Don't compute for LSTMs, which don't have behavior dependent on a single eigenvalue\n",
        "    if isinstance(self.rnn, RNNLayer):\n",
        "      ##############################################################################\n",
        "      # TODO: Set W = the weight which most affects exploding/vanishing gradients  #\n",
        "      # Hint: Call module.weight or module.bias on the module you want to use      #\n",
        "      # If you used a single Linear layer, slice a square matrix from it.          #\n",
        "      ##############################################################################\n",
        "      ##############################################################################\n",
        "      #                               END OF YOUR CODE                             #\n",
        "      ##############################################################################\n",
        "      biggest_eig = biggest_eig_magnitude(W)\n",
        "      print(f' Biggest eigenvalue magnitude: {biggest_eig:.3}')\n",
        "\n",
        "    # Run model\n",
        "    pred, h = self.model(self.x)\n",
        "    loss = loss_fn(pred, self.y, self.last_timestep_only)\n",
        "    n_steps = len(h[0])\n",
        "\n",
        "    plt_1, plt_2 = self.plots\n",
        "\n",
        "    # Plot the hidden state magnitude\n",
        "    max_h = th.linalg.norm(h[0], dim=-1).detach().cpu().numpy()\n",
        "    print('Max H', ' '.join([f'{num:.3}' for num in max_h]))\n",
        "    plt_1.set_data(np.arange(1, n_steps + 1), np.array(max_h))\n",
        "    # Compute the gradient for the loss wrt the stored hidden states\n",
        "    # Gradients are plotted backward since we go from later layers to earlier\n",
        "    grads = [th.linalg.norm(num).item() for num in th.autograd.grad(loss, self.rnn.h_list)][::-1]\n",
        "    print('gradients d Loss/d h_t', ' '.join([f'{num:.3}' for num in grads]))\n",
        "    # Add 1e-6 since it throws an error for gradients near 0\n",
        "    plt_2.set_data(np.arange(n_steps), np.array(grads) + 1e-6)\n",
        "    self.fig.canvas.draw_idle()\n",
        "\n",
        "  def create_visualization(self):\n",
        "    # Include sliders for relevant quantities\n",
        "    self.plot_visuals()\n",
        "    ip = interactive(self.update_plots,\n",
        "                    weight_val=widgets.FloatSlider(value=0, min=-5, max=5, step=.05, description=\"weight_scale\", layout=Layout(width='100%')),\n",
        "                    bias_val=widgets.FloatSlider(value=0, min=-5, max=5, step=.05, description=\"bias_scale\", layout=Layout(width='100%')),\n",
        "                    )\n",
        "    return ip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiTaiihtZb_q"
      },
      "source": [
        "Adjust the sliders rescale the weight and bias parameters in the RNN. Observe the effect on exploding and vanishing gradients.\n",
        "\n",
        "Parameters to try varying:\n",
        "*   nonlinearity\n",
        "*   last_target_only\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G_B0NEvEAAp"
      },
      "outputs": [],
      "source": [
        "hidden_size = 16\n",
        "nonlinearity = lambda x: x  # options include lambda x: x (no nonlinearity), nn.functional.relu, and th.tanh\n",
        "last_target_only = True\n",
        "rnn = RNNLayer(1, hidden_size, nonlinearity=nonlinearity)\n",
        "gv = GradientVisualizer(rnn, last_target_only)\n",
        "gv.create_visualization()\n",
        "\n",
        "# If for some reason the slider doesn't work for you, try calling gv.update_plots\n",
        "# with various values for weight and bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qPcibFiyVsk"
      },
      "source": [
        "# Problem 1.H: Implementing a single-layer LSTM\n",
        "\n",
        "Hint: consider creating parameters using Pytorch's [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear). You can implement this with either one Linear layer or two for each equation. If you use two, remember that you only need to include a bias term for one of the linear layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5P--0vXGTwU"
      },
      "outputs": [],
      "source": [
        "class LSTMLayer(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    \"\"\"\n",
        "    Initialize a single LSTM layer.\n",
        "\n",
        "    Inputs:\n",
        "    - input_size: Data input feature dimension\n",
        "    - hidden_size: RNN hidden state size (also the output feature dimension)\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize any parameters your class needs.                          #\n",
        "    ##############################################################################\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    LSTM forward pass\n",
        "\n",
        "    Inputs:\n",
        "    - x: input tensor (B, seq_len, input_size)\n",
        "\n",
        "    Returns:\n",
        "    - all_h: tensor of size (B, seq_len, hidden_size) containing hidden states\n",
        "             produced for each timestep\n",
        "    - (h_last, c_last): hidden and cell states from the last timestep, each of\n",
        "             size (B, hidden_size)\n",
        "    \"\"\"\n",
        "    h_list = []\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the LSTM forward step                                      #\n",
        "    # 1. Initialize the hidden and cell states with zeros                        #\n",
        "    # 2. Roll out the LSTM over the sequence, populating h_list along the way    #\n",
        "    # 3. Return the appropriate outputs                                          #\n",
        "    ##############################################################################\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "    # h_list should now contain all hidden states, each of size (B, hidden_size)\n",
        "    # We will store the hidden states so we can analyze their gradients later\n",
        "    self.store_h_for_grad(h_list)\n",
        "    all_h = th.stack(h_list, dim=1)\n",
        "\n",
        "    return all_h, (h_last, c_last)\n",
        "\n",
        "  def store_h_for_grad(self, h_list):\n",
        "    \"\"\"\n",
        "    Store input list and allow gradient computation for all list elements\n",
        "    \"\"\"\n",
        "    for h in h_list:\n",
        "      h.retain_grad()\n",
        "    self.h_list = h_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30VuigbiL-7O"
      },
      "source": [
        "### Test Cases\n",
        "\n",
        "A correct implementation should have errors < 1e-4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6Oj2DZeKVcc"
      },
      "outputs": [],
      "source": [
        "lstm = LSTMLayer(2, 3)\n",
        "lstm.load_state_dict({k: v * 0 - .1 for k, v in lstm.state_dict().items()})\n",
        "data = th.FloatTensor([[[.1, .15], [.2, .25], [.3, .35], [.4, .45]], [[-.1, -1.5], [-.2, -2.5], [-.3, -3.5], [-.4, -.45]]])\n",
        "expected_all_h = th.FloatTensor([[[-0.0273, -0.0273, -0.0273],\n",
        "         [-0.0420, -0.0420, -0.0420],\n",
        "         [-0.0514, -0.0514, -0.0514],\n",
        "         [-0.0583, -0.0583, -0.0583]],\n",
        "\n",
        "        [[ 0.0159,  0.0159,  0.0159],\n",
        "         [ 0.0568,  0.0568,  0.0568],\n",
        "         [ 0.1142,  0.1142,  0.1142],\n",
        "         [ 0.0369,  0.0369,  0.0369]]])\n",
        "expected_last_h = th.FloatTensor([[-0.0583, -0.0583, -0.0583],\n",
        "        [ 0.0369,  0.0369,  0.0369]])\n",
        "expected_last_c = th.FloatTensor([[-0.1280, -0.1280, -0.1280],\n",
        "        [ 0.0759,  0.0759,  0.0759]])\n",
        "all_h, (last_h, last_c) = lstm(data)\n",
        "assert all_h.shape == (2, 4, 3)\n",
        "assert last_h.shape == last_c.shape == (2, 3)\n",
        "print(f'Max error all_h: {th.max(th.abs(expected_all_h - all_h)).item()}')\n",
        "print(f'Max error last_h: {th.max(th.abs(expected_last_h - last_h)).item()}')\n",
        "print(f'Max error last_h: {th.max(th.abs(expected_last_c - last_c)).item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj0Vf8l_yn9L"
      },
      "source": [
        "## Problem 1.8b: Analyzing gradient flow through a single-layer LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgnrwyrjlmCy"
      },
      "outputs": [],
      "source": [
        "hidden_size = 3\n",
        "last_target_only = True\n",
        "rnn = LSTMLayer(1, hidden_size)\n",
        "gv = GradientVisualizer(rnn, last_target_only)\n",
        "gv.create_visualization()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAhVHPaEy4x8"
      },
      "source": [
        "# Problem 1.K: Making a multi-layer RNN and LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doMWE3eeouQE"
      },
      "source": [
        "## 1.K.i: Implementing multi-layer models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns8-s9-MMInz"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    \"\"\"\n",
        "    Initialize a multilayer RNN\n",
        "\n",
        "    Inputs:\n",
        "    - input_size: Data input feature dimension\n",
        "    - hidden_size: hidden state size (also the output feature dimension)\n",
        "    - num_layers: number of layers\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    assert num_layers >= 1\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize any parameters your class needs.                          #\n",
        "    # Consider using nn.ModuleList or nn.ModuleDict.                             #\n",
        "    ##############################################################################\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Multilayer RNN forward pass\n",
        "\n",
        "    Inputs:\n",
        "    - x: input tensor (B, seq_len, input_size)\n",
        "\n",
        "    Returns:\n",
        "    - last_layer_h: tensor of size (B, seq_len, hidden_size) containing the\n",
        "             outputs produced for each timestep from the last layer\n",
        "    - last_step_h: all hidden states from the last step (num_layers, B, hidden_size)\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the RNN forward step                                       #\n",
        "    ##############################################################################\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return last_layer_h, last_step_h\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    \"\"\"\n",
        "    Initialize a multilayer LSTM\n",
        "\n",
        "    Inputs:\n",
        "    - input_size: Data input feature dimension\n",
        "    - hidden_size: hidden state size (also the output feature dimension)\n",
        "    - num_layers: number of layers\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    assert num_layers >= 1\n",
        "    layers = [LSTMLayer(input_size, hidden_size)]\n",
        "    for i in range(num_layers - 1):\n",
        "      layers.append(LSTMLayer(hidden_size, hidden_size))\n",
        "    self.layers = nn.ModuleList(layers)\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "  def forward(self, x, hc0=None):\n",
        "    \"\"\"\n",
        "    Multilayer LSTM forward pass\n",
        "\n",
        "    Inputs:\n",
        "    - x: input tensor (B, seq_len, input_size)\n",
        "\n",
        "    Returns:\n",
        "    - last_layer_h: tensor of size (B, seq_len, hidden_size) containing the\n",
        "             outputs produced for each timestep from the last layer\n",
        "    - (last_step_h, last_step_c): all hidden and cell states from the last step\n",
        "            size (num_layers, B, hidden_size)\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the LSTM forward step                                       #\n",
        "    ##############################################################################\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return last_layer_h, (last_step_h, last_step_c)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o2noEJyN8c0"
      },
      "source": [
        "### Test Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mskU70eN8mq"
      },
      "outputs": [],
      "source": [
        "rnn = RNN(2, 3, 1)\n",
        "rnn.load_state_dict({k: v * 0 - .1 for k, v in rnn.state_dict().items()})\n",
        "data = th.FloatTensor([[[.1, .15], [.2, .25], [.3, .35], [.4, .45]], [[-.1, -1.5], [-.2, -2.5], [-.3, -3.5], [-.4, -.45]]])\n",
        "expected_all_h = th.FloatTensor([[[-0.1244, -0.1244, -0.1244],\n",
        "         [-0.1073, -0.1073, -0.1073],\n",
        "         [-0.1320, -0.1320, -0.1320],\n",
        "         [-0.1444, -0.1444, -0.1444]],\n",
        "\n",
        "        [[ 0.0599,  0.0599,  0.0599],\n",
        "         [ 0.1509,  0.1509,  0.1509],\n",
        "         [ 0.2305,  0.2305,  0.2305],\n",
        "         [-0.0840, -0.0840, -0.0840]]])\n",
        "expected_last_h = th.FloatTensor([[[-0.1444, -0.1444, -0.1444],\n",
        "         [-0.0840, -0.0840, -0.0840]]])\n",
        "all_h, last_h = rnn(data)\n",
        "assert all_h.shape == expected_all_h.shape\n",
        "assert last_h.shape == expected_last_h.shape\n",
        "print(f'Max error all_h: {th.max(th.abs(expected_all_h - all_h)).item()}')\n",
        "print(f'Max error last_h: {th.max(th.abs(expected_last_h - last_h)).item()}')\n",
        "\n",
        "rnn = RNN(2, 3, 2)\n",
        "rnn.load_state_dict({k: v * 0 - .1 for k, v in rnn.state_dict().items()})\n",
        "data = th.FloatTensor([[[.1, .15], [.2, .25], [.3, .35], [.4, .45]], [[-.1, -1.5], [-.2, -2.5], [-.3, -3.5], [-.4, -.45]]])\n",
        "expected_all_h = th.FloatTensor([[[-0.0626, -0.0626, -0.0626],\n",
        "         [-0.0490, -0.0490, -0.0490],\n",
        "         [-0.0457, -0.0457, -0.0457],\n",
        "         [-0.0430, -0.0430, -0.0430]],\n",
        "        [[-0.1174, -0.1174, -0.1174],\n",
        "         [-0.1096, -0.1096, -0.1096],\n",
        "         [-0.1354, -0.1354, -0.1354],\n",
        "         [-0.0342, -0.0342, -0.0342]]])\n",
        "expected_last_h = th.FloatTensor([[[-0.1444, -0.1444, -0.1444],\n",
        "         [-0.0840, -0.0840, -0.0840]],\n",
        "        [[-0.0430, -0.0430, -0.0430],\n",
        "         [-0.0342, -0.0342, -0.0342]]])\n",
        "all_h, last_h = rnn(data)\n",
        "assert all_h.shape == (2, 4, 3)\n",
        "assert last_h.shape == (2, 2, 3)\n",
        "print(f'Max error all_h: {th.max(th.abs(expected_all_h - all_h)).item()}')\n",
        "print(f'Max error last_h: {th.max(th.abs(expected_last_h - last_h)).item()}')\n",
        "\n",
        "\n",
        "lstm = LSTM(2, 3, 1)\n",
        "lstm.load_state_dict({k: v * 0 - .1 for k, v in lstm.state_dict().items()})\n",
        "data = th.FloatTensor([[[.1, .15], [.2, .25], [.3, .35], [.4, .45]], [[-.1, -1.5], [-.2, -2.5], [-.3, -3.5], [-.4, -.45]]])\n",
        "expected_all_h = th.FloatTensor([[[-0.0273, -0.0273, -0.0273],\n",
        "         [-0.0420, -0.0420, -0.0420],\n",
        "         [-0.0514, -0.0514, -0.0514],\n",
        "         [-0.0583, -0.0583, -0.0583]],\n",
        "\n",
        "        [[ 0.0159,  0.0159,  0.0159],\n",
        "         [ 0.0568,  0.0568,  0.0568],\n",
        "         [ 0.1142,  0.1142,  0.1142],\n",
        "         [ 0.0369,  0.0369,  0.0369]]])\n",
        "expected_last_h = th.FloatTensor([[[-0.0583, -0.0583, -0.0583],\n",
        "         [ 0.0369,  0.0369,  0.0369]]])\n",
        "expected_last_c = th.FloatTensor([[[-0.1280, -0.1280, -0.1280],\n",
        "         [ 0.0759,  0.0759,  0.0759]]])\n",
        "all_h, (last_h, last_c) = lstm(data)\n",
        "assert all_h.shape == (2, 4, 3)\n",
        "assert last_h.shape == last_c.shape == (1, 2, 3)\n",
        "print(f'Max error all_h: {th.max(th.abs(expected_all_h - all_h)).item()}')\n",
        "print(f'Max error last_h: {th.max(th.abs(expected_last_h - last_h)).item()}')\n",
        "print(f'Max error last_c: {th.max(th.abs(expected_last_c - last_c)).item()}')\n",
        "\n",
        "\n",
        "lstm = LSTM(2, 3, 3)\n",
        "lstm.load_state_dict({k: v * 0 - .1 for k, v in lstm.state_dict().items()})\n",
        "data = th.FloatTensor([[[.1, .15], [.2, .25], [.3, .35], [.4, .45]], [[-.1, -1.5], [-.2, -2.5], [-.3, -3.5], [-.4, -.45]]])\n",
        "expected_all_h = th.FloatTensor([[[-0.0212, -0.0212, -0.0212],\n",
        "         [-0.0296, -0.0296, -0.0296],\n",
        "         [-0.0329, -0.0329, -0.0329],\n",
        "         [-0.0343, -0.0343, -0.0343]],\n",
        "        [[-0.0211, -0.0211, -0.0211],\n",
        "         [-0.0291, -0.0291, -0.0291],\n",
        "         [-0.0320, -0.0320, -0.0320],\n",
        "         [-0.0332, -0.0332, -0.0332]]])\n",
        "expected_last_h = th.FloatTensor([[[-0.0583, -0.0583, -0.0583],\n",
        "         [ 0.0369,  0.0369,  0.0369]],\n",
        "        [[-0.0320, -0.0320, -0.0320],\n",
        "         [-0.0430, -0.0430, -0.0430]],\n",
        "        [[-0.0343, -0.0343, -0.0343],\n",
        "         [-0.0332, -0.0332, -0.0332]]])\n",
        "expected_last_c = th.FloatTensor([[[-0.1280, -0.1280, -0.1280],\n",
        "         [ 0.0759,  0.0759,  0.0759]],\n",
        "        [[-0.0666, -0.0666, -0.0666],\n",
        "         [-0.0907, -0.0907, -0.0907]],\n",
        "        [[-0.0716, -0.0716, -0.0716],\n",
        "         [-0.0693, -0.0693, -0.0693]]])\n",
        "all_h, (last_h, last_c) = lstm(data)\n",
        "assert all_h.shape == (2, 4, 3)\n",
        "assert last_h.shape == last_c.shape == (3, 2, 3)\n",
        "\n",
        "print(f'Max error all_h: {th.max(th.abs(expected_all_h - all_h)).item()}')\n",
        "print(f'Max error last_h: {th.max(th.abs(expected_last_h - last_h)).item()}')\n",
        "print(f'Max error last_c: {th.max(th.abs(expected_last_c - last_c)).item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQriJsNirJp2"
      },
      "source": [
        "## 1.K.ii: Training your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O5rRLJTrI-A"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, num_batches, last_timestep_only, seq_len=10, batch_size=32):\n",
        "  model\n",
        "  model.train()\n",
        "\n",
        "  losses = []\n",
        "  from tqdm import tqdm\n",
        "  t = tqdm(range(0, num_batches))\n",
        "  for i in t:\n",
        "      data, labels = generate_batch(seq_len=seq_len, batch_size=batch_size)\n",
        "      pred, h = model(data)\n",
        "      loss = loss_fn(pred, labels, last_timestep_only)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if i % 100 == 0:\n",
        "          t.set_description(f\"Batch: {i} Loss: {np.mean(losses[-10:])}\")\n",
        "  return losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlkbHdA_r-1N"
      },
      "outputs": [],
      "source": [
        "def train_all(hidden_size, lr, num_batches, last_timestep_only):\n",
        "  input_size = 1\n",
        "  rnn_1_layer = RecurrentRegressionModel(RNN(input_size, hidden_size, 1))\n",
        "  lstm_1_layer = RecurrentRegressionModel(LSTM(input_size, hidden_size, 1))\n",
        "  rnn_2_layer = RecurrentRegressionModel(RNN(input_size, hidden_size, 2))\n",
        "  lstm_2_layer = RecurrentRegressionModel(LSTM(input_size, hidden_size, 2))\n",
        "  models = [rnn_1_layer, lstm_1_layer, rnn_2_layer, lstm_2_layer]\n",
        "  model_names = ['rnn_1_layer', 'lstm_1_layer', 'rnn_2_layer', 'lstm_2_layer']\n",
        "\n",
        "  losses = []\n",
        "  for model in models:\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss = train(model, optimizer, num_batches, last_timestep_only)\n",
        "    losses.append(loss)\n",
        "\n",
        "  # visualize the results\n",
        "  fig, ax1 = plt.subplots(1)\n",
        "  for loss in losses:\n",
        "    ax1.plot(loss)\n",
        "  ax1.legend(model_names)\n",
        "  plt.show()\n",
        "\n",
        "  batch_size = 4\n",
        "  x, y = generate_batch(seq_len=10, batch_size=batch_size)\n",
        "  preds_list = [model(x)[0] for model in models]\n",
        "  for i in range(batch_size):\n",
        "    fig, ax1 = plt.subplots(1)\n",
        "    ax1.plot(x[i, :, 0])\n",
        "    if last_timestep_only:\n",
        "      ax1.plot(np.arange(10), [y[i, -1].item()] * 10, 'bo')\n",
        "    else:\n",
        "      ax1.plot(y[i, :, 0], 'bo')\n",
        "    for pred in preds_list:\n",
        "      if last_timestep_only:\n",
        "        ax1.plot(np.arange(10), [pred[i, -1, 0].detach().cpu().numpy()] * 10)\n",
        "      else:\n",
        "        ax1.plot(pred[i, :, 0].detach().cpu().numpy())\n",
        "    ax1.legend(['x', 'y'] + model_names)\n",
        "    plt.show()\n",
        "  return models, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HNCFFiRyere"
      },
      "outputs": [],
      "source": [
        "hidden_size = 32\n",
        "lr = 1e-4\n",
        "num_batches = 5000\n",
        "last_timestep_only = False\n",
        "\n",
        "th.manual_seed(0)\n",
        "predict_all_models, predict_all_losses = train_all(hidden_size, lr, num_batches, last_timestep_only)\n",
        "last_timestep_only = True\n",
        "predict_one_models, predict_one_losses = train_all(hidden_size, lr, num_batches, last_timestep_only)"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [
        "7WmHtmlVul1z",
        "usA4sa1CDzOs",
        "tbZtognVqQEO",
        "wHVeC-hCSA6S",
        "VC9LWlnOKyKi",
        "bZZ26gO5wYAH",
        "GLLCIL7iH4CE",
        "OLV9IqVOWnoC",
        "7qPcibFiyVsk",
        "30VuigbiL-7O",
        "Fj0Vf8l_yn9L",
        "doMWE3eeouQE",
        "8o2noEJyN8c0"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}